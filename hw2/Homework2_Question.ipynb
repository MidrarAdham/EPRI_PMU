{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8c3cb4",
   "metadata": {},
   "source": [
    "# Homework 2:  Improve Baseline CNN model and compute metrics for assessing the performance of the CNN-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a0f64",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "\n",
    "    1. Train Basic Model (From Homework 1)\n",
    "\n",
    "    2. Saving and Loading Model\n",
    "    \n",
    "    3. Metrics Access Performance\n",
    "    \n",
    "    4. Hyper-parameter Tuning\n",
    "    \n",
    "    5. Overfitting Prevention\n",
    "    \n",
    "    6. Compare Performance of Basic and Improved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5214f",
   "metadata": {},
   "source": [
    "## 1. Train Basic Model (From Homework 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorflow, which is a framework for deep learning.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "# Load numpy library as \"np\", which can handle large matrices and provides some mathematical functions.\n",
    "import numpy as np \n",
    "# Load pandas as \"pd\", which is useful when working with data tables. \n",
    "import pandas as pd \n",
    "# Load random, which provide some randomize functions.\n",
    "import random\n",
    "# Load a function pyplot as \"plt\" to plot figures.\n",
    "import matplotlib.pyplot as plt\n",
    "# Load functions to calculate precision, and recall\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Setup the random seed for reproducibility\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The root directory of the pmuBAGE data\n",
    "pmuBAGE_data_dir = \"pmuBAGE/data\"\n",
    "\n",
    "# Number of the tensor for voltage and frequency\n",
    "voltage_tensor_number = 31\n",
    "frequency_tensor_number = 21\n",
    "\n",
    "# Load each tensors of voltage events and concatenate them as a big tensor.\n",
    "voltage_tensor_list = []\n",
    "for idx in range(voltage_tensor_number):\n",
    "    voltage_sub_tensor = np.load(f\"{pmuBAGE_data_dir}/voltage/voltage_{idx}.npy\")\n",
    "    voltage_tensor_list.append(voltage_sub_tensor)\n",
    "voltage_tensor = np.concatenate(voltage_tensor_list, axis=0)\n",
    "\n",
    "\n",
    "# Load each tensors of frequency events and concatenate them as a big tensor.\n",
    "frequency_tensor_list = []\n",
    "for idx in range(frequency_tensor_number):\n",
    "    frequency_sub_tensor = np.load(f\"{pmuBAGE_data_dir}/frequency/frequency_{idx}.npy\")\n",
    "    frequency_tensor_list.append(frequency_sub_tensor)\n",
    "frequency_tensor = np.concatenate(frequency_tensor_list, axis=0)\n",
    "\n",
    "# Transpose the big tensor as (event_idx, timestamp, PMU_idx, measurements)\n",
    "voltage_tensor = np.transpose(voltage_tensor, (0, 3, 2, 1))\n",
    "frequency_tensor = np.transpose(frequency_tensor, (0, 3, 2, 1))\n",
    "\n",
    "# Print the shape of the voltage event\n",
    "print(voltage_tensor.shape)\n",
    "print(frequency_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use standardization to pre-process the pmu time series data.\n",
    "    Input  -> two original tensors: voltage_tensor, frequency_tensor\n",
    "    Output -> two standardized tensors: voltage_tensor_standardized, frequency_tensor_standardized\n",
    "    Requirement Details: \n",
    "        The tensor shape is (number_of_event, timestamps (600), pmus (100), measurements (4))\n",
    "        For each time sequence (Single pmu measurement sequence, 600 timestamps), standardize them by Z-Score\n",
    "        z-score = (x - mean) / std\n",
    "\"\"\"\n",
    "\n",
    "# Voltage tensor\n",
    "\n",
    "voltage_mean = np.mean(voltage_tensor, axis=1)\n",
    "voltage_mean = np.expand_dims(voltage_mean, axis=1)\n",
    "voltage_std = np.std(voltage_tensor, axis=1)\n",
    "voltage_std = np.expand_dims(voltage_std, axis=1)\n",
    "voltage_tensor_standardized = np.nan_to_num((voltage_tensor - voltage_mean) / voltage_std)\n",
    "\n",
    "# Frequency tensor\n",
    "\n",
    "frequency_mean = np.mean(frequency_tensor, axis=1)\n",
    "frequency_mean = np.expand_dims(frequency_mean, axis=1)\n",
    "frequency_std = np.std(frequency_tensor, axis=1)\n",
    "frequency_std = np.expand_dims(frequency_std, axis=1)\n",
    "frequency_tensor_standardized = np.nan_to_num((frequency_tensor - frequency_mean) / frequency_std)\n",
    "\n",
    "print(voltage_tensor_standardized.shape)\n",
    "print(frequency_tensor_standardized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071115fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of the classes\n",
    "num_classes = 2\n",
    "\n",
    "# Number of the voltage and frequency events in the dataset\n",
    "n_voltage = voltage_tensor_standardized.shape[0]\n",
    "n_frequency = frequency_tensor_standardized.shape[0]\n",
    "\n",
    "# Define the labels\n",
    "# Voltage events' label is defined as: 0\n",
    "voltage_label = np.array([0] * n_voltage)\n",
    "# Frequency events' label is defined as: 1\n",
    "frequency_label = np.array([1] * n_frequency)\n",
    "\n",
    "\"\"\"\n",
    "    Implement the one-hot encoding on the lablel of of the voltage and frequency event labels.\n",
    "    Input  -> Original voltage and frequency labels (voltage_label, frequency_label)\n",
    "    Output -> One-hot encoded voltage and frequency labels (voltage_label_onehot, frequency_label_onthot)\n",
    "    Voltage label: \"0\" -> \"[1, 0]\"\n",
    "    Frequency label: \"1\" -> \"[0, 1]\"\n",
    "    You can use any library or tool for doing this\n",
    "\"\"\"\n",
    "\n",
    "voltage_label_onehot = tf.keras.utils.to_categorical(voltage_label, num_classes=num_classes)\n",
    "frequency_label_onthot = tf.keras.utils.to_categorical(frequency_label, num_classes=num_classes)\n",
    "\n",
    "# Should be [1, 0]\n",
    "print(voltage_label_onehot[0])\n",
    "# Should be [0, 1]\n",
    "print(frequency_label_onthot[0])\n",
    "# Should be (620, 2)\n",
    "print(voltage_label_onehot.shape)\n",
    "# Should be (84, 2)\n",
    "print(frequency_label_onthot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ba982",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage_tensor_standarded_permuted = voltage_tensor_standardized[np.random.permutation(n_voltage)]\n",
    "frequency_tensor_standarded_permuted = frequency_tensor_standardized[np.random.permutation(n_frequency)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b58c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the data to train and test\n",
    "train_portion = 0.7\n",
    "\n",
    "# Samples\n",
    "X_voltage = voltage_tensor_standarded_permuted\n",
    "X_frequency = frequency_tensor_standarded_permuted\n",
    "# Labels\n",
    "y_voltage = voltage_label_onehot\n",
    "y_frequency = frequency_label_onthot\n",
    "\n",
    "\"\"\"\n",
    "    Seperate the samples and labels to train and test datasets.\n",
    "    70% of the voltage and frequency samples and labels are combined as training dataset\n",
    "    30% remainings are combined as testing dataset\n",
    "    Input  -> X_voltage, X_frequency, y_voltage, y_frequency\n",
    "    Output -> X_train, y_train, X_test, y_test\n",
    "        X_train contains 70% of the X_voltage and X_frequency\n",
    "        y_train contains 70% of the y_voltage and y_frequency\n",
    "        X_test contains 30% of the X_voltage and X_frequency\n",
    "        y_test contains 30% of the y_voltage and y_frequency\n",
    "\"\"\"\n",
    "\n",
    "# X_train\n",
    "X_train_voltage = X_voltage[:int(n_voltage * train_portion)] \n",
    "X_train_frequency = X_frequency[:int(n_frequency * train_portion)]\n",
    "X_train = np.concatenate([X_train_voltage, X_train_frequency], axis=0)\n",
    "\n",
    "# y_train\n",
    "y_train_voltage = y_voltage[:int(n_voltage * train_portion)] \n",
    "y_train_frequency = y_frequency[:int(n_frequency * train_portion)]\n",
    "y_train = np.concatenate([y_train_voltage, y_train_frequency], axis=0)\n",
    "\n",
    "# X_test\n",
    "X_test_voltage = X_voltage[int(n_voltage * train_portion):] \n",
    "X_test_frequency = X_frequency[int(n_frequency * train_portion):]\n",
    "X_test = np.concatenate([X_test_voltage, X_test_frequency], axis=0)\n",
    "\n",
    "# y_test\n",
    "y_test_voltage = y_voltage[int(n_voltage * train_portion):] \n",
    "y_test_frequency = y_frequency[int(n_frequency * train_portion):]\n",
    "y_test = np.concatenate([y_test_voltage, y_test_frequency], axis=0)\n",
    "\n",
    "# Should be (492, 600, 100, 4)\n",
    "print(X_train.shape)\n",
    "# Should be (492, 2)\n",
    "print(y_train.shape)\n",
    "# Should be (212, 600, 100, 4)\n",
    "print(X_test.shape)\n",
    "# Should be (212, 2)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494860b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "        Add more laybers in the model, at least three convolusional layers.\n",
    "        Then add the Flatten and Dense layers to make the output same with the number of classes.\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(600, 100, 4)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Define the Loss function\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Define the optimizer and learning rate\n",
    "lr = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "# Compile the neural network model\n",
    "model.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "\n",
    "# Train the neural network\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
    "\n",
    "# Evaluate the neural network\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"The accuracy of the neural network on the test dataset is: {accuracy}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7d767",
   "metadata": {},
   "source": [
    "## 2. Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32189909",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\"\"\"\n",
    "    Save trained model to file for future application or further fine-tuning train. \n",
    "\"\"\"\n",
    "\n",
    "# Write the code to save the trained model to file.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Load the model from the file and compile it.\n",
    "\"\"\"\n",
    "\n",
    "# Write the code to load the model from file and compile it.\n",
    "model = ...\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5114e9",
   "metadata": {},
   "source": [
    "## 3. Metrics Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59196fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test samples and labels, and get the model's prediction on test data.\n",
    "X_test = X_test\n",
    "y_test = y_test\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77503f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\"\"\"\n",
    "    Homework 1, only calculate the accuracy of the whole dataset.\n",
    "    In this task, you required to calculate the accuracy, precision, recall, and F1-score.\n",
    "\"\"\"\n",
    "\n",
    "# Accuracy\n",
    "accuracy = ...\n",
    "\n",
    "# Precision\n",
    "precision = ...\n",
    "\n",
    "# Recall\n",
    "recall = ...\n",
    "\n",
    "# F1-Score\n",
    "f1 = ...\n",
    "\n",
    "print(f\"The accuracy is: {accuracy}.\")\n",
    "print(f\"The precision is: {precision}.\")\n",
    "print(f\"The recall is: {recall}.\")\n",
    "print(f\"The f1 score is: {f1}.\")\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40ad63",
   "metadata": {},
   "source": [
    "## 4. Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Seperate the Training Dataset to Training (80%) and Validation (20%).\n",
    "    Perform the hyper-parameter tuning to find the best parameter combination.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "val_portaton = 0.2\n",
    "\n",
    "n_voltage_train = int(n_voltage * train_portion)\n",
    "n_frequency_train = int(n_frequency * train_portion)\n",
    "\n",
    "X_train_voltage = X_voltage[:int(n_voltage * train_portion)] \n",
    "X_train_frequency = X_frequency[:int(n_frequency * train_portion)]\n",
    "X_train = np.concatenate([X_train_voltage, X_train_frequency], axis=0)\n",
    "\n",
    "# X_val_hp\n",
    "X_val_hp_voltage = X_train[:int(n_voltage_train * val_portaton)]\n",
    "X_val_hp_frequency = X_train[n_voltage_train: n_voltage_train + int(n_frequency_train * val_portaton)]\n",
    "X_val_hp = np.concatenate([X_val_hp_voltage, X_val_hp_frequency], axis=0)\n",
    "\n",
    "# y_val_hp\n",
    "y_val_hp_voltage = y_train[:int(n_voltage_train * val_portaton)]\n",
    "y_val_hp_frequency = y_train[n_voltage_train: n_voltage_train + int(n_frequency_train * val_portaton)]\n",
    "y_val_hp = np.concatenate([y_val_hp_voltage, y_val_hp_frequency], axis=0)\n",
    "\n",
    "\n",
    "# X_train_hp\n",
    "X_train_hp_voltage = X_train[int(n_voltage_train * val_portaton): n_voltage_train]\n",
    "X_train_hp_frequency = X_train[n_voltage_train + int(n_frequency_train * val_portaton):]\n",
    "X_train_hp = np.concatenate([X_train_hp_voltage, X_train_hp_frequency], axis=0)\n",
    "\n",
    "# y_train_hp\n",
    "y_train_hp_voltage = y_train[int(n_voltage_train * val_portaton): n_voltage_train]\n",
    "y_train_hp_frequency = y_train[n_voltage_train + int(n_frequency_train * val_portaton):]\n",
    "y_train_hp = np.concatenate([y_train_hp_voltage, y_train_hp_frequency], axis=0)\n",
    "\n",
    "\n",
    "# Should be (492, 600, 100, 4)\n",
    "print(X_train_hp.shape)\n",
    "# Should be (492, 2)\n",
    "print(y_train_hp.shape)\n",
    "# Should be (212, 600, 100, 4)\n",
    "print(X_val_hp.shape)\n",
    "# Should be (212, 2)\n",
    "print(y_val_hp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371668f5-d6bc-4db8-9034-ab524c9b6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the random seed for reproducibility\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a508dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learning_rates = [0.001, 0.01]\n",
    "batch_sizes = [16, 32]\n",
    "training_epochs = [10, 20]\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Try different combination of the hyper-parameters.\n",
    "    Train on training dataset, test on validation dataset.\n",
    "    Choose the best hyper-parameter combination to train the final improved model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "best_hyperparameter = {\"learning_rate\": 0, \"batch_size\": 0, \"training_epoch\": 0}\n",
    "best_accuracy_val = 0.0\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for training_epoch in training_epochs:\n",
    "            print(f\"Current hyper-parameters: learning_rate: {learning_rate}, batch_size: {batch_size}, training_epoch: {training_epoch}.\")\n",
    "            \n",
    "            \"\"\"\n",
    "                Train the model under hyper-parameter setting, and evaluate over validation dataset.\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\" Filling code below \"\"\"\n",
    "            \n",
    "            # build model\n",
    "            model_tuning = build_model()\n",
    "            # Train the model with the above hyper-parameters\n",
    "\n",
    "            \n",
    "            \"\"\" End Filling \"\"\"\n",
    "    \n",
    "            # Evaluate the hyper-parameter tuning neural network\n",
    "            loss_val, accuracy_val = model_tuning.evaluate(X_val_hp, y_val_hp)\n",
    "            print(f\"Current validation accuracy is: {accuracy_val}.\\n\")\n",
    "            \n",
    "            if accuracy_val > best_accuracy_val:\n",
    "                best_accuracy_val = accuracy_val\n",
    "                best_hyperparameter[\"learning_rate\"] = learning_rate\n",
    "                best_hyperparameter[\"batch_size\"] = batch_size\n",
    "                best_hyperparameter[\"training_epoch\"] = training_epoch\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            \n",
    "print(f\"best validation accuracy is: {best_accuracy_val}\")\n",
    "print(f\"best hyper-parameter setting is: {best_hyperparameter}\")\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea655f64",
   "metadata": {},
   "source": [
    "## 5. Overfitting Prevention\n",
    "    Early Stopping to Prevent the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53437133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best hyper-parameter from previous tuning. (You may change them based on previous results)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "training_epoch = 20\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Use the early stopping to prevent the overfitting.\n",
    "    Useful resource: https://keras.io/api/callbacks/early_stopping/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Build model\n",
    "model_improved = build_model()\n",
    "# Define the loss function\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "# Compile the model\n",
    "model_improved.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "\n",
    "\"\"\" Filling code below \"\"\"\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = ...\n",
    "\n",
    "\"\"\" End Filling \"\"\"\n",
    "\n",
    "# Train the model with the tuned hyper-parameters and early-stopping.\n",
    "history = model_improved.fit(X_train_hp, y_train_hp, validation_data=(X_val_hp, y_val_hp), \n",
    "                             epochs=training_epoch, batch_size=batch_size, callbacks=[early_stopping])\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b3bcf",
   "metadata": {},
   "source": [
    "## 6. Compare Performance of Basic and Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, F1-score of the basic model\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "precision = precision_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n",
    "\n",
    "print(\"Performance of the basic model.\")\n",
    "print(f\"The accuracy is: {accuracy}.\")\n",
    "print(f\"The precision is: {precision}.\")\n",
    "print(f\"The recall is: {recall}.\")\n",
    "print(f\"The f1 score is: {f1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, F1-score of the improved model\n",
    "\n",
    "y_pred_improved = model_improved.predict(X_test)\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1))\n",
    "precision = precision_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1), average='macro')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1), average='macro')\n",
    "\n",
    "print(\"Performance of the improved model.\")\n",
    "print(f\"The accuracy is: {accuracy}.\")\n",
    "print(f\"The precision is: {precision}.\")\n",
    "print(f\"The recall is: {recall}.\")\n",
    "print(f\"The f1 score is: {f1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2af259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710e7a7-c75c-4a5b-ac0f-e6999d96b224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
