{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8c3cb4",
   "metadata": {},
   "source": [
    "# Homework 2:  Improve Baseline CNN model and compute metrics for assessing the performance of the CNN-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a0f64",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "\n",
    "    1. Train Basic Model (From Homework 1)\n",
    "\n",
    "    2. Saving and Loading Model\n",
    "    \n",
    "    3. Metrics Access Performance\n",
    "    \n",
    "    4. Hyper-parameter Tuning\n",
    "    \n",
    "    5. Overfitting Prevention\n",
    "    \n",
    "    6. Compare Performance of Basic and Improved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5214f",
   "metadata": {},
   "source": [
    "## 1. Train Basic Model (From Homework 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36e42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorflow, which is a framework for deep learning.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "# Load numpy library as \"np\", which can handle large matrices and provides some mathematical functions.\n",
    "import numpy as np \n",
    "# Load pandas as \"pd\", which is useful when working with data tables. \n",
    "import pandas as pd \n",
    "# Load random, which provide some randomize functions.\n",
    "import random\n",
    "# Load a function pyplot as \"plt\" to plot figures.\n",
    "import matplotlib.pyplot as plt\n",
    "# Load functions to calculate precision, and recall\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Setup the random seed for reproducibility\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabcf8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 600, 100, 4)\n",
      "(84, 600, 100, 4)\n"
     ]
    }
   ],
   "source": [
    "# The root directory of the pmuBAGE data\n",
    "pmuBAGE_data_dir = \"../data/pmuBAGE/data\"\n",
    "\n",
    "# Number of the tensor for voltage and frequency\n",
    "voltage_tensor_number = 31\n",
    "frequency_tensor_number = 21\n",
    "\n",
    "# Load each tensors of voltage events and concatenate them as a big tensor.\n",
    "voltage_tensor_list = []\n",
    "for idx in range(voltage_tensor_number):\n",
    "    voltage_sub_tensor = np.load(f\"{pmuBAGE_data_dir}/voltage/voltage_{idx}.npy\")\n",
    "    voltage_tensor_list.append(voltage_sub_tensor)\n",
    "voltage_tensor = np.concatenate(voltage_tensor_list, axis=0)\n",
    "\n",
    "\n",
    "# Load each tensors of frequency events and concatenate them as a big tensor.\n",
    "frequency_tensor_list = []\n",
    "for idx in range(frequency_tensor_number):\n",
    "    frequency_sub_tensor = np.load(f\"{pmuBAGE_data_dir}/frequency/frequency_{idx}.npy\")\n",
    "    frequency_tensor_list.append(frequency_sub_tensor)\n",
    "frequency_tensor = np.concatenate(frequency_tensor_list, axis=0)\n",
    "\n",
    "# Transpose the big tensor as (event_idx, timestamp, PMU_idx, measurements)\n",
    "voltage_tensor = np.transpose(voltage_tensor, (0, 3, 2, 1))\n",
    "frequency_tensor = np.transpose(frequency_tensor, (0, 3, 2, 1))\n",
    "\n",
    "# Print the shape of the voltage event\n",
    "print(voltage_tensor.shape)\n",
    "print(frequency_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c49b797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 600, 100, 4)\n",
      "(84, 600, 100, 4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Use standardization to pre-process the pmu time series data.\n",
    "    Input  -> two original tensors: voltage_tensor, frequency_tensor\n",
    "    Output -> two standardized tensors: voltage_tensor_standardized, frequency_tensor_standardized\n",
    "    Requirement Details: \n",
    "        The tensor shape is (number_of_event, timestamps (600), pmus (100), measurements (4))\n",
    "        For each time sequence (Single pmu measurement sequence, 600 timestamps), standardize them by Z-Score\n",
    "        z-score = (x - mean) / std\n",
    "\"\"\"\n",
    "\n",
    "# Voltage tensor\n",
    "\n",
    "voltage_mean = np.mean(voltage_tensor, axis=1)\n",
    "voltage_mean = np.expand_dims(voltage_mean, axis=1)\n",
    "voltage_std = np.std(voltage_tensor, axis=1)\n",
    "voltage_std = np.expand_dims(voltage_std, axis=1)\n",
    "voltage_tensor_standardized = np.nan_to_num((voltage_tensor - voltage_mean) / voltage_std)\n",
    "\n",
    "# Frequency tensor\n",
    "\n",
    "frequency_mean = np.mean(frequency_tensor, axis=1)\n",
    "frequency_mean = np.expand_dims(frequency_mean, axis=1)\n",
    "frequency_std = np.std(frequency_tensor, axis=1)\n",
    "frequency_std = np.expand_dims(frequency_std, axis=1)\n",
    "frequency_tensor_standardized = np.nan_to_num((frequency_tensor - frequency_mean) / frequency_std)\n",
    "\n",
    "print(voltage_tensor_standardized.shape)\n",
    "print(frequency_tensor_standardized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "071115fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "[0. 1.]\n",
      "(620, 2)\n",
      "(84, 2)\n"
     ]
    }
   ],
   "source": [
    "# Number of the classes\n",
    "num_classes = 2\n",
    "\n",
    "# Number of the voltage and frequency events in the dataset\n",
    "n_voltage = voltage_tensor_standardized.shape[0]\n",
    "n_frequency = frequency_tensor_standardized.shape[0]\n",
    "\n",
    "# Define the labels\n",
    "# Voltage events' label is defined as: 0\n",
    "voltage_label = np.array([0] * n_voltage)\n",
    "# Frequency events' label is defined as: 1\n",
    "frequency_label = np.array([1] * n_frequency)\n",
    "\n",
    "\"\"\"\n",
    "    Implement the one-hot encoding on the lablel of of the voltage and frequency event labels.\n",
    "    Input  -> Original voltage and frequency labels (voltage_label, frequency_label)\n",
    "    Output -> One-hot encoded voltage and frequency labels (voltage_label_onehot, frequency_label_onthot)\n",
    "    Voltage label: \"0\" -> \"[1, 0]\"\n",
    "    Frequency label: \"1\" -> \"[0, 1]\"\n",
    "    You can use any library or tool for doing this\n",
    "\"\"\"\n",
    "\n",
    "voltage_label_onehot = tf.keras.utils.to_categorical(voltage_label, num_classes=num_classes)\n",
    "frequency_label_onthot = tf.keras.utils.to_categorical(frequency_label, num_classes=num_classes)\n",
    "\n",
    "# Should be [1, 0]\n",
    "print(voltage_label_onehot[0])\n",
    "# Should be [0, 1]\n",
    "print(frequency_label_onthot[0])\n",
    "# Should be (620, 2)\n",
    "print(voltage_label_onehot.shape)\n",
    "# Should be (84, 2)\n",
    "print(frequency_label_onthot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8ba982",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage_tensor_standarded_permuted = voltage_tensor_standardized[np.random.permutation(n_voltage)]\n",
    "frequency_tensor_standarded_permuted = frequency_tensor_standardized[np.random.permutation(n_frequency)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b58c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492, 600, 100, 4)\n",
      "(492, 2)\n",
      "(212, 600, 100, 4)\n",
      "(212, 2)\n"
     ]
    }
   ],
   "source": [
    "# Seperate the data to train and test\n",
    "train_portion = 0.7\n",
    "\n",
    "# Samples\n",
    "X_voltage = voltage_tensor_standarded_permuted\n",
    "X_frequency = frequency_tensor_standarded_permuted\n",
    "# Labels\n",
    "y_voltage = voltage_label_onehot\n",
    "y_frequency = frequency_label_onthot\n",
    "\n",
    "\"\"\"\n",
    "    Seperate the samples and labels to train and test datasets.\n",
    "    70% of the voltage and frequency samples and labels are combined as training dataset\n",
    "    30% remainings are combined as testing dataset\n",
    "    Input  -> X_voltage, X_frequency, y_voltage, y_frequency\n",
    "    Output -> X_train, y_train, X_test, y_test\n",
    "        X_train contains 70% of the X_voltage and X_frequency\n",
    "        y_train contains 70% of the y_voltage and y_frequency\n",
    "        X_test contains 30% of the X_voltage and X_frequency\n",
    "        y_test contains 30% of the y_voltage and y_frequency\n",
    "\"\"\"\n",
    "\n",
    "# X_train\n",
    "X_train_voltage = X_voltage[:int(n_voltage * train_portion)] \n",
    "X_train_frequency = X_frequency[:int(n_frequency * train_portion)]\n",
    "X_train = np.concatenate([X_train_voltage, X_train_frequency], axis=0)\n",
    "\n",
    "# y_train\n",
    "y_train_voltage = y_voltage[:int(n_voltage * train_portion)] \n",
    "y_train_frequency = y_frequency[:int(n_frequency * train_portion)]\n",
    "y_train = np.concatenate([y_train_voltage, y_train_frequency], axis=0)\n",
    "\n",
    "# X_test\n",
    "X_test_voltage = X_voltage[int(n_voltage * train_portion):] \n",
    "X_test_frequency = X_frequency[int(n_frequency * train_portion):]\n",
    "X_test = np.concatenate([X_test_voltage, X_test_frequency], axis=0)\n",
    "\n",
    "# y_test\n",
    "y_test_voltage = y_voltage[int(n_voltage * train_portion):] \n",
    "y_test_frequency = y_frequency[int(n_frequency * train_portion):]\n",
    "y_test = np.concatenate([y_test_voltage, y_test_frequency], axis=0)\n",
    "\n",
    "# Should be (492, 600, 100, 4)\n",
    "print(X_train.shape)\n",
    "# Should be (492, 2)\n",
    "print(y_train.shape)\n",
    "# Should be (212, 600, 100, 4)\n",
    "print(X_test.shape)\n",
    "# Should be (212, 2)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "494860b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "        Add more laybers in the model, at least three convolusional layers.\n",
    "        Then add the Flatten and Dense layers to make the output same with the number of classes.\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(600, 100, 4)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Define the Loss function\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Define the optimizer and learning rate\n",
    "lr = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "# Compile the neural network model\n",
    "model.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "\n",
    "# Train the neural network\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
    "\n",
    "# Evaluate the neural network\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"The accuracy of the neural network on the test dataset is: {accuracy}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7d767",
   "metadata": {},
   "source": [
    "## 2. Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32189909",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\"\"\"\n",
    "    Save trained model to file for future application or further fine-tuning train. \n",
    "\"\"\"\n",
    "\n",
    "# Write the code to save the trained model to file.\n",
    "model.save('midrar_model.h5')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Load the model from the file and compile it.\n",
    "\"\"\"\n",
    "lr = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Write the code to load the model from file and compile it.\n",
    "model = models.load_model('midrar_model.h5')\n",
    "model.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5114e9",
   "metadata": {},
   "source": [
    "## 3. Metrics Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b08bf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 598, 98, 32)       1184      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 299, 49, 32)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 297, 47, 32)       9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 148, 23, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 146, 21, 32)       9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 73, 10, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 23360)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                747552    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 767298 (2.93 MB)\n",
      "Trainable params: 767298 (2.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59196fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 185ms/step\n",
      "[[0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]\n",
      " [0.79797757 0.20202245]]\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Get the test samples and labels, and get the model's prediction on test data.\n",
    "X_test = X_test\n",
    "y_test = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)\n",
    "print('\\n\\n----\\n\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77503f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.8773584905660378.\n",
      "The precision is: 0.7697579209683162.\n",
      "The recall is: 0.8773584905660378.\n",
      "The f1 score is: 0.8200436142979046.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midrar/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\"\"\"\n",
    "    Homework 1, only calculate the accuracy of the whole dataset.\n",
    "    In this task, you required to calculate the accuracy, precision, recall, and F1-score.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "# The argmax returns the highest probability in the provided y_predict variable in the previous block cell. I can't run the accuracy directly because I'd be comparing binary values to numerical values. Therefore,\n",
    "# converting one of the variables to binary would solve the problem, I think. \n",
    "y_prediction = tf.argmax(y_pred, axis=1)\n",
    "y_testing = tf.argmax(y_test, axis=1)\n",
    "accuracy = accuracy_score(y_true=y_testing, y_pred=y_prediction)\n",
    "\n",
    "# Precision\n",
    "\n",
    "# This is kind of confusing. But here is what I tried, without the average attribute, I would get a zero precision. As per scikit learn documentation (link provided in the slides), the average is specified for multi\n",
    "# classification problem. What I don't understand is why wouldn't I use the other attributes, such as 'binary'.\n",
    "\n",
    "\n",
    "precision = precision_score(y_true=y_testing, y_pred=y_prediction, average='weighted')\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true=y_testing, y_pred=y_prediction, average='weighted')\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(y_true=y_testing, y_pred=y_prediction, average='weighted')\n",
    "\n",
    "print(f\"The accuracy is: {accuracy}.\")\n",
    "print(f\"The precision is: {precision}.\")\n",
    "print(f\"The recall is: {recall}.\")\n",
    "print(f\"The f1 score is: {f1}.\")\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40ad63",
   "metadata": {},
   "source": [
    "## 4. Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaecb8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 600, 100, 4)\n",
      "(395, 2)\n",
      "(97, 600, 100, 4)\n",
      "(97, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Seperate the Training Dataset to Training (80%) and Validation (20%).\n",
    "    Perform the hyper-parameter tuning to find the best parameter combination.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "val_portaton = 0.2\n",
    "\n",
    "n_voltage_train = int(n_voltage * train_portion)\n",
    "n_frequency_train = int(n_frequency * train_portion)\n",
    "\n",
    "X_train_voltage = X_voltage[:int(n_voltage * train_portion)] \n",
    "X_train_frequency = X_frequency[:int(n_frequency * train_portion)]\n",
    "X_train = np.concatenate([X_train_voltage, X_train_frequency], axis=0)\n",
    "\n",
    "# X_val_hp\n",
    "X_val_hp_voltage = X_train[:int(n_voltage_train * val_portaton)]\n",
    "X_val_hp_frequency = X_train[n_voltage_train: n_voltage_train + int(n_frequency_train * val_portaton)]\n",
    "X_val_hp = np.concatenate([X_val_hp_voltage, X_val_hp_frequency], axis=0)\n",
    "\n",
    "# y_val_hp\n",
    "y_val_hp_voltage = y_train[:int(n_voltage_train * val_portaton)]\n",
    "y_val_hp_frequency = y_train[n_voltage_train: n_voltage_train + int(n_frequency_train * val_portaton)]\n",
    "y_val_hp = np.concatenate([y_val_hp_voltage, y_val_hp_frequency], axis=0)\n",
    "\n",
    "\n",
    "# X_train_hp\n",
    "X_train_hp_voltage = X_train[int(n_voltage_train * val_portaton): n_voltage_train]\n",
    "X_train_hp_frequency = X_train[n_voltage_train + int(n_frequency_train * val_portaton):]\n",
    "X_train_hp = np.concatenate([X_train_hp_voltage, X_train_hp_frequency], axis=0)\n",
    "\n",
    "# y_train_hp\n",
    "y_train_hp_voltage = y_train[int(n_voltage_train * val_portaton): n_voltage_train]\n",
    "y_train_hp_frequency = y_train[n_voltage_train + int(n_frequency_train * val_portaton):]\n",
    "y_train_hp = np.concatenate([y_train_hp_voltage, y_train_hp_frequency], axis=0)\n",
    "\n",
    "\n",
    "# Should be (492, 600, 100, 4)\n",
    "print(X_train_hp.shape)\n",
    "# Should be (492, 2)\n",
    "print(y_train_hp.shape)\n",
    "# Should be (212, 600, 100, 4)\n",
    "print(X_val_hp.shape)\n",
    "# Should be (212, 2)\n",
    "print(y_val_hp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "371668f5-d6bc-4db8-9034-ab524c9b6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the random seed for reproducibility\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a508dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current hyper-parameters: learning_rate: 0.001, batch_size: 16, training_epoch: 10.\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 14s 544ms/step - loss: 0.4414 - categorical_accuracy: 0.8658 - val_loss: 0.1959 - val_categorical_accuracy: 0.8866\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 14s 542ms/step - loss: 0.1999 - categorical_accuracy: 0.9443 - val_loss: 0.1509 - val_categorical_accuracy: 0.9691\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 14s 576ms/step - loss: 0.1376 - categorical_accuracy: 0.9494 - val_loss: 0.1454 - val_categorical_accuracy: 0.9691\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 13s 539ms/step - loss: 0.1044 - categorical_accuracy: 0.9722 - val_loss: 0.1014 - val_categorical_accuracy: 0.9691\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 14s 563ms/step - loss: 0.0654 - categorical_accuracy: 0.9722 - val_loss: 0.1717 - val_categorical_accuracy: 0.9588\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 14s 566ms/step - loss: 0.0393 - categorical_accuracy: 0.9899 - val_loss: 0.1104 - val_categorical_accuracy: 0.9794\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 13s 535ms/step - loss: 0.0173 - categorical_accuracy: 0.9975 - val_loss: 0.1263 - val_categorical_accuracy: 0.9691\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 14s 555ms/step - loss: 0.0040 - categorical_accuracy: 1.0000 - val_loss: 0.1431 - val_categorical_accuracy: 0.9691\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 14s 554ms/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.1717 - val_categorical_accuracy: 0.9691\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 14s 566ms/step - loss: 5.7774e-04 - categorical_accuracy: 1.0000 - val_loss: 0.1669 - val_categorical_accuracy: 0.9691\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.1669 - categorical_accuracy: 0.9691\n",
      "Current validation accuracy is: 0.969072163105011.\n",
      "\n",
      "Current hyper-parameters: learning_rate: 0.001, batch_size: 16, training_epoch: 20.\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 15s 572ms/step - loss: 0.6280 - categorical_accuracy: 0.8506 - val_loss: 0.1712 - val_categorical_accuracy: 0.9072\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 14s 549ms/step - loss: 0.1926 - categorical_accuracy: 0.9443 - val_loss: 0.1483 - val_categorical_accuracy: 0.9691\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 13s 539ms/step - loss: 0.1767 - categorical_accuracy: 0.9241 - val_loss: 0.2124 - val_categorical_accuracy: 0.8969\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 14s 576ms/step - loss: 0.1538 - categorical_accuracy: 0.9696 - val_loss: 0.1619 - val_categorical_accuracy: 0.9588\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 14s 557ms/step - loss: 0.1220 - categorical_accuracy: 0.9494 - val_loss: 0.1139 - val_categorical_accuracy: 0.9794\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 14s 561ms/step - loss: 0.0729 - categorical_accuracy: 0.9823 - val_loss: 0.1472 - val_categorical_accuracy: 0.9691\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 14s 543ms/step - loss: 0.0499 - categorical_accuracy: 0.9848 - val_loss: 0.1448 - val_categorical_accuracy: 0.9794\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 14s 568ms/step - loss: 0.0311 - categorical_accuracy: 0.9924 - val_loss: 0.1280 - val_categorical_accuracy: 0.9897\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 14s 555ms/step - loss: 0.0188 - categorical_accuracy: 0.9975 - val_loss: 0.2987 - val_categorical_accuracy: 0.9588\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 14s 543ms/step - loss: 0.0147 - categorical_accuracy: 0.9975 - val_loss: 0.1532 - val_categorical_accuracy: 0.9897\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 14s 563ms/step - loss: 0.0065 - categorical_accuracy: 1.0000 - val_loss: 0.2587 - val_categorical_accuracy: 0.9588\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 14s 551ms/step - loss: 0.0155 - categorical_accuracy: 0.9949 - val_loss: 0.1410 - val_categorical_accuracy: 0.9897\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 14s 568ms/step - loss: 0.0097 - categorical_accuracy: 0.9949 - val_loss: 0.2141 - val_categorical_accuracy: 0.9794\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 14s 542ms/step - loss: 0.0031 - categorical_accuracy: 1.0000 - val_loss: 0.1794 - val_categorical_accuracy: 0.9794\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 13s 521ms/step - loss: 0.0014 - categorical_accuracy: 1.0000 - val_loss: 0.1735 - val_categorical_accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 13s 536ms/step - loss: 6.3996e-04 - categorical_accuracy: 1.0000 - val_loss: 0.2123 - val_categorical_accuracy: 0.9794\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 14s 559ms/step - loss: 3.7001e-04 - categorical_accuracy: 1.0000 - val_loss: 0.2054 - val_categorical_accuracy: 0.9794\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 13s 534ms/step - loss: 2.8263e-04 - categorical_accuracy: 1.0000 - val_loss: 0.2197 - val_categorical_accuracy: 0.9794\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 13s 540ms/step - loss: 2.2653e-04 - categorical_accuracy: 1.0000 - val_loss: 0.2117 - val_categorical_accuracy: 0.9794\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 13s 524ms/step - loss: 1.8973e-04 - categorical_accuracy: 1.0000 - val_loss: 0.2224 - val_categorical_accuracy: 0.9794\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 0.2224 - categorical_accuracy: 0.9794\n",
      "Current validation accuracy is: 0.9793814420700073.\n",
      "\n",
      "Current hyper-parameters: learning_rate: 0.001, batch_size: 32, training_epoch: 10.\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.7114 - categorical_accuracy: 0.8633 - val_loss: 0.2337 - val_categorical_accuracy: 0.8866\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1898 - categorical_accuracy: 0.8810 - val_loss: 0.1626 - val_categorical_accuracy: 0.8866\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1920 - categorical_accuracy: 0.9139 - val_loss: 0.1563 - val_categorical_accuracy: 0.9691\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.1622 - categorical_accuracy: 0.9494 - val_loss: 0.1454 - val_categorical_accuracy: 0.9691\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1471 - categorical_accuracy: 0.9494 - val_loss: 0.1582 - val_categorical_accuracy: 0.9072\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1360 - categorical_accuracy: 0.9443 - val_loss: 0.1560 - val_categorical_accuracy: 0.9381\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.1298 - categorical_accuracy: 0.9620 - val_loss: 0.1585 - val_categorical_accuracy: 0.9278\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1250 - categorical_accuracy: 0.9443 - val_loss: 0.1434 - val_categorical_accuracy: 0.9588\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1094 - categorical_accuracy: 0.9646 - val_loss: 0.1657 - val_categorical_accuracy: 0.9485\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.1025 - categorical_accuracy: 0.9696 - val_loss: 0.1757 - val_categorical_accuracy: 0.9485\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.1757 - categorical_accuracy: 0.9485\n",
      "Current validation accuracy is: 0.9484536051750183.\n",
      "\n",
      "Current hyper-parameters: learning_rate: 0.001, batch_size: 32, training_epoch: 20.\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.2584 - categorical_accuracy: 0.8354 - val_loss: 0.2945 - val_categorical_accuracy: 0.8866\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 13s 996ms/step - loss: 0.2459 - categorical_accuracy: 0.8810 - val_loss: 0.1958 - val_categorical_accuracy: 0.8866\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.1818 - categorical_accuracy: 0.8810 - val_loss: 0.1822 - val_categorical_accuracy: 0.8866\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1640 - categorical_accuracy: 0.9367 - val_loss: 0.1726 - val_categorical_accuracy: 0.9588\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1552 - categorical_accuracy: 0.9519 - val_loss: 0.1741 - val_categorical_accuracy: 0.9485\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.1426 - categorical_accuracy: 0.9494 - val_loss: 0.1782 - val_categorical_accuracy: 0.9381\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 0.1362 - categorical_accuracy: 0.9570 - val_loss: 0.1818 - val_categorical_accuracy: 0.9588\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.1142 - categorical_accuracy: 0.9722 - val_loss: 0.1262 - val_categorical_accuracy: 0.9588\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.0916 - categorical_accuracy: 0.9646 - val_loss: 0.3103 - val_categorical_accuracy: 0.9072\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.1146 - categorical_accuracy: 0.9443 - val_loss: 0.1355 - val_categorical_accuracy: 0.9691\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.0583 - categorical_accuracy: 0.9848 - val_loss: 0.1476 - val_categorical_accuracy: 0.9691\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.0553 - categorical_accuracy: 0.9848 - val_loss: 0.1600 - val_categorical_accuracy: 0.9588\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.0446 - categorical_accuracy: 0.9873 - val_loss: 0.1584 - val_categorical_accuracy: 0.9588\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.0447 - categorical_accuracy: 0.9899 - val_loss: 0.1363 - val_categorical_accuracy: 0.9485\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.0386 - categorical_accuracy: 0.9899 - val_loss: 0.1251 - val_categorical_accuracy: 0.9485\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.0335 - categorical_accuracy: 0.9873 - val_loss: 0.1418 - val_categorical_accuracy: 0.9588\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.0434 - categorical_accuracy: 0.9873 - val_loss: 0.1484 - val_categorical_accuracy: 0.9588\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.0335 - categorical_accuracy: 0.9899 - val_loss: 0.1586 - val_categorical_accuracy: 0.9588\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.0289 - categorical_accuracy: 0.9924 - val_loss: 0.1574 - val_categorical_accuracy: 0.9588\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 0.0296 - categorical_accuracy: 0.9924 - val_loss: 0.1602 - val_categorical_accuracy: 0.9588\n",
      "4/4 [==============================] - 1s 131ms/step - loss: 0.1602 - categorical_accuracy: 0.9588\n",
      "Current validation accuracy is: 0.9587628841400146.\n",
      "\n",
      "Current hyper-parameters: learning_rate: 0.01, batch_size: 16, training_epoch: 10.\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 14s 550ms/step - loss: 2.4468 - categorical_accuracy: 0.8709 - val_loss: 0.2268 - val_categorical_accuracy: 0.8866\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 14s 554ms/step - loss: 0.4027 - categorical_accuracy: 0.8835 - val_loss: 0.2377 - val_categorical_accuracy: 0.8866\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 13s 539ms/step - loss: 0.6332 - categorical_accuracy: 0.8861 - val_loss: 0.5233 - val_categorical_accuracy: 0.8866\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 13s 520ms/step - loss: 0.4665 - categorical_accuracy: 0.8810 - val_loss: 0.4086 - val_categorical_accuracy: 0.8866\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 13s 512ms/step - loss: 0.3943 - categorical_accuracy: 0.8810 - val_loss: 0.3759 - val_categorical_accuracy: 0.8866\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 13s 518ms/step - loss: 0.3747 - categorical_accuracy: 0.8810 - val_loss: 0.3623 - val_categorical_accuracy: 0.8866\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 14s 545ms/step - loss: 0.3677 - categorical_accuracy: 0.8810 - val_loss: 0.3574 - val_categorical_accuracy: 0.8866\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 13s 521ms/step - loss: 0.3652 - categorical_accuracy: 0.8810 - val_loss: 0.3555 - val_categorical_accuracy: 0.8866\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 13s 523ms/step - loss: 0.3642 - categorical_accuracy: 0.8810 - val_loss: 0.3548 - val_categorical_accuracy: 0.8866\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 13s 514ms/step - loss: 0.3641 - categorical_accuracy: 0.8810 - val_loss: 0.3542 - val_categorical_accuracy: 0.8866\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.3542 - categorical_accuracy: 0.8866\n",
      "Current validation accuracy is: 0.8865979313850403.\n",
      "\n",
      "Current hyper-parameters: learning_rate: 0.01, batch_size: 16, training_epoch: 20.\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 19s 588ms/step - loss: 1.0535 - categorical_accuracy: 0.8101 - val_loss: 0.3516 - val_categorical_accuracy: 0.8866\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 13s 534ms/step - loss: 0.2857 - categorical_accuracy: 0.8886 - val_loss: 0.3022 - val_categorical_accuracy: 0.9278\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 14s 553ms/step - loss: 0.1863 - categorical_accuracy: 0.9519 - val_loss: 0.2331 - val_categorical_accuracy: 0.9278\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 13s 538ms/step - loss: 0.1234 - categorical_accuracy: 0.9696 - val_loss: 0.1424 - val_categorical_accuracy: 0.9485\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 13s 527ms/step - loss: 0.1159 - categorical_accuracy: 0.9595 - val_loss: 0.2232 - val_categorical_accuracy: 0.9588\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 13s 539ms/step - loss: 0.0681 - categorical_accuracy: 0.9873 - val_loss: 0.6861 - val_categorical_accuracy: 0.9175\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 13s 534ms/step - loss: 0.1188 - categorical_accuracy: 0.9570 - val_loss: 0.4212 - val_categorical_accuracy: 0.9485\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 13s 520ms/step - loss: 0.0581 - categorical_accuracy: 0.9797 - val_loss: 0.5734 - val_categorical_accuracy: 0.9691\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 13s 523ms/step - loss: 0.0327 - categorical_accuracy: 0.9924 - val_loss: 0.4009 - val_categorical_accuracy: 0.9485\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 14s 547ms/step - loss: 0.0272 - categorical_accuracy: 0.9899 - val_loss: 0.4020 - val_categorical_accuracy: 0.9691\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 13s 527ms/step - loss: 0.0077 - categorical_accuracy: 0.9949 - val_loss: 0.6304 - val_categorical_accuracy: 0.9485\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 13s 525ms/step - loss: 0.1001 - categorical_accuracy: 0.9696 - val_loss: 0.8005 - val_categorical_accuracy: 0.9485\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 13s 528ms/step - loss: 0.0617 - categorical_accuracy: 0.9848 - val_loss: 0.1660 - val_categorical_accuracy: 0.9588\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 13s 534ms/step - loss: 0.0491 - categorical_accuracy: 0.9899 - val_loss: 0.5224 - val_categorical_accuracy: 0.9485\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 14s 571ms/step - loss: 0.0297 - categorical_accuracy: 0.9899 - val_loss: 0.2028 - val_categorical_accuracy: 0.9588\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 14s 546ms/step - loss: 0.0092 - categorical_accuracy: 0.9949 - val_loss: 0.4302 - val_categorical_accuracy: 0.9588\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 14s 541ms/step - loss: 0.0021 - categorical_accuracy: 1.0000 - val_loss: 0.3402 - val_categorical_accuracy: 0.9588\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 13s 527ms/step - loss: 1.7797e-04 - categorical_accuracy: 1.0000 - val_loss: 0.3768 - val_categorical_accuracy: 0.9588\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 14s 565ms/step - loss: 9.1132e-05 - categorical_accuracy: 1.0000 - val_loss: 0.3917 - val_categorical_accuracy: 0.9588\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 13s 529ms/step - loss: 6.5602e-05 - categorical_accuracy: 1.0000 - val_loss: 0.4035 - val_categorical_accuracy: 0.9588\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.4035 - categorical_accuracy: 0.9588\n",
      "Current validation accuracy is: 0.9587628841400146.\n",
      "\n",
      "Current hyper-parameters: learning_rate: 0.01, batch_size: 32, training_epoch: 10.\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 15s 1s/step - loss: 10.9569 - categorical_accuracy: 0.8380 - val_loss: 0.5880 - val_categorical_accuracy: 0.8866\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.5254 - categorical_accuracy: 0.8810 - val_loss: 0.4469 - val_categorical_accuracy: 0.8866\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.4195 - categorical_accuracy: 0.8810 - val_loss: 0.3765 - val_categorical_accuracy: 0.8866\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3771 - categorical_accuracy: 0.8810 - val_loss: 0.3564 - val_categorical_accuracy: 0.8866\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3649 - categorical_accuracy: 0.8810 - val_loss: 0.3540 - val_categorical_accuracy: 0.8866\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 13s 990ms/step - loss: 0.3651 - categorical_accuracy: 0.8810 - val_loss: 0.3536 - val_categorical_accuracy: 0.8866\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3654 - categorical_accuracy: 0.8810 - val_loss: 0.3536 - val_categorical_accuracy: 0.8866\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3653 - categorical_accuracy: 0.8810 - val_loss: 0.3536 - val_categorical_accuracy: 0.8866\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3654 - categorical_accuracy: 0.8810 - val_loss: 0.3539 - val_categorical_accuracy: 0.8866\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3653 - categorical_accuracy: 0.8810 - val_loss: 0.3537 - val_categorical_accuracy: 0.8866\n",
      "4/4 [==============================] - 1s 140ms/step - loss: 0.3537 - categorical_accuracy: 0.8866\n",
      "Current validation accuracy is: 0.8865979313850403.\n",
      "\n",
      "Current hyper-parameters: learning_rate: 0.01, batch_size: 32, training_epoch: 20.\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 8.6767 - categorical_accuracy: 0.7316 - val_loss: 0.6489 - val_categorical_accuracy: 0.8866\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.6160 - categorical_accuracy: 0.8810 - val_loss: 0.5707 - val_categorical_accuracy: 0.8866\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.5448 - categorical_accuracy: 0.8810 - val_loss: 0.5065 - val_categorical_accuracy: 0.8866\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.4903 - categorical_accuracy: 0.8810 - val_loss: 0.4602 - val_categorical_accuracy: 0.8866\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.4494 - categorical_accuracy: 0.8810 - val_loss: 0.4286 - val_categorical_accuracy: 0.8866\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.4230 - categorical_accuracy: 0.8810 - val_loss: 0.4048 - val_categorical_accuracy: 0.8866\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.4041 - categorical_accuracy: 0.8810 - val_loss: 0.3887 - val_categorical_accuracy: 0.8866\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3911 - categorical_accuracy: 0.8810 - val_loss: 0.3779 - val_categorical_accuracy: 0.8866\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3824 - categorical_accuracy: 0.8810 - val_loss: 0.3709 - val_categorical_accuracy: 0.8866\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3771 - categorical_accuracy: 0.8810 - val_loss: 0.3655 - val_categorical_accuracy: 0.8866\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3729 - categorical_accuracy: 0.8810 - val_loss: 0.3618 - val_categorical_accuracy: 0.8866\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3703 - categorical_accuracy: 0.8810 - val_loss: 0.3594 - val_categorical_accuracy: 0.8866\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3686 - categorical_accuracy: 0.8810 - val_loss: 0.3579 - val_categorical_accuracy: 0.8866\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3672 - categorical_accuracy: 0.8810 - val_loss: 0.3568 - val_categorical_accuracy: 0.8866\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3670 - categorical_accuracy: 0.8810 - val_loss: 0.3556 - val_categorical_accuracy: 0.8866\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3660 - categorical_accuracy: 0.8810 - val_loss: 0.3552 - val_categorical_accuracy: 0.8866\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3657 - categorical_accuracy: 0.8810 - val_loss: 0.3550 - val_categorical_accuracy: 0.8866\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3654 - categorical_accuracy: 0.8810 - val_loss: 0.3546 - val_categorical_accuracy: 0.8866\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.3653 - categorical_accuracy: 0.8810 - val_loss: 0.3545 - val_categorical_accuracy: 0.8866\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.3652 - categorical_accuracy: 0.8810 - val_loss: 0.3543 - val_categorical_accuracy: 0.8866\n",
      "4/4 [==============================] - 1s 136ms/step - loss: 0.3543 - categorical_accuracy: 0.8866\n",
      "Current validation accuracy is: 0.8865979313850403.\n",
      "\n",
      "best validation accuracy is: 0.9793814420700073\n",
      "best hyper-parameter setting is: {'learning_rate': 0.001, 'batch_size': 16, 'training_epoch': 20}\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "learning_rates = [0.001, 0.01]\n",
    "batch_sizes = [16, 32]\n",
    "training_epochs = [10, 20]\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Try different combination of the hyper-parameters.\n",
    "    Train on training dataset, test on validation dataset.\n",
    "    Choose the best hyper-parameter combination to train the final improved model.\n",
    "\"\"\"\n",
    "\n",
    "# I tried different parameters are in the cell block. \n",
    "\n",
    "best_hyperparameter = {\"learning_rate\": 0, \"batch_size\": 0, \"training_epoch\": 0}\n",
    "best_accuracy_val = 0.0\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for training_epoch in training_epochs:\n",
    "            print(f\"Current hyper-parameters: learning_rate: {learning_rate}, batch_size: {batch_size}, training_epoch: {training_epoch}.\")\n",
    "            \n",
    "            \"\"\"\n",
    "                Train the model under hyper-parameter setting, and evaluate over validation dataset.\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\" Filling code below \"\"\"\n",
    "            \n",
    "            # build model\n",
    "            model_tuning = build_model()\n",
    "            # Train the model with the above hyper-parameters\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "            model_tuning.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "\n",
    "            history = model_tuning.fit(X_train_hp, y_train_hp, epochs=training_epoch, batch_size=batch_size, validation_data=(X_val_hp, y_val_hp))\n",
    "\n",
    "            \"\"\" End Filling \"\"\"\n",
    "    \n",
    "            # Evaluate the hyper-parameter tuning neural network\n",
    "            loss_val, accuracy_val = model_tuning.evaluate(X_val_hp, y_val_hp)\n",
    "            print(f\"Current validation accuracy is: {accuracy_val}.\\n\")\n",
    "            \n",
    "            if accuracy_val > best_accuracy_val:\n",
    "                best_accuracy_val = accuracy_val\n",
    "                best_hyperparameter[\"learning_rate\"] = learning_rate\n",
    "                best_hyperparameter[\"batch_size\"] = batch_size\n",
    "                best_hyperparameter[\"training_epoch\"] = training_epoch\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            \n",
    "print(f\"best validation accuracy is: {best_accuracy_val}\")\n",
    "print(f\"best hyper-parameter setting is: {best_hyperparameter}\")\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d770f",
   "metadata": {},
   "source": [
    "## In the next cell block, I am trying different hyperparameters to see if this network can be improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea492f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "learning_rates = [0.0001, 0.003]\n",
    "batch_sizes = [10, 20]\n",
    "training_epochs = [15, 25]\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Try different combination of the hyper-parameters.\n",
    "    Train on training dataset, test on validation dataset.\n",
    "    Choose the best hyper-parameter combination to train the final improved model.\n",
    "\"\"\"\n",
    "\n",
    "# I tried different parameters are in the cell block. \n",
    "\n",
    "best_hyperparameter = {\"learning_rate\": 0, \"batch_size\": 0, \"training_epoch\": 0}\n",
    "best_accuracy_val = 0.0\n",
    "\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for training_epoch in training_epochs:\n",
    "            print(f\"Current hyper-parameters: learning_rate: {learning_rate}, batch_size: {batch_size}, training_epoch: {training_epoch}.\")\n",
    "            \n",
    "            \"\"\"\n",
    "                Train the model under hyper-parameter setting, and evaluate over validation dataset.\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\" Filling code below \"\"\"\n",
    "            \n",
    "            # build model\n",
    "            model_tuning = build_model()\n",
    "            # Train the model with the above hyper-parameters\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "            model_tuning.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "\n",
    "            history = model_tuning.fit(X_train_hp, y_train_hp, epochs=training_epoch, batch_size=batch_size, validation_data=(X_val_hp, y_val_hp))\n",
    "\n",
    "            \"\"\" End Filling \"\"\"\n",
    "    \n",
    "            # Evaluate the hyper-parameter tuning neural network\n",
    "            loss_val, accuracy_val = model_tuning.evaluate(X_val_hp, y_val_hp)\n",
    "            print(f\"Current validation accuracy is: {accuracy_val}.\\n\")\n",
    "            \n",
    "            if accuracy_val > best_accuracy_val:\n",
    "                best_accuracy_val = accuracy_val\n",
    "                best_hyperparameter[\"learning_rate\"] = learning_rate\n",
    "                best_hyperparameter[\"batch_size\"] = batch_size\n",
    "                best_hyperparameter[\"training_epoch\"] = training_epoch\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            \n",
    "print(f\"best validation accuracy is: {best_accuracy_val}\")\n",
    "print(f\"best hyper-parameter setting is: {best_hyperparameter}\")\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b387dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/31 [==============================] - 17s 525ms/step - loss: 0.4777 - categorical_accuracy: 0.8882\n",
      "Epoch 2/20\n",
      "31/31 [==============================] - 16s 519ms/step - loss: 0.1516 - categorical_accuracy: 0.9451\n",
      "Epoch 3/20\n",
      "31/31 [==============================] - 16s 505ms/step - loss: 0.1077 - categorical_accuracy: 0.9715\n",
      "Epoch 4/20\n",
      "31/31 [==============================] - 16s 513ms/step - loss: 0.1141 - categorical_accuracy: 0.9634\n",
      "Epoch 5/20\n",
      "31/31 [==============================] - 15s 496ms/step - loss: 0.0768 - categorical_accuracy: 0.9776\n",
      "Epoch 6/20\n",
      "31/31 [==============================] - 15s 487ms/step - loss: 0.0677 - categorical_accuracy: 0.9736\n",
      "Epoch 7/20\n",
      "31/31 [==============================] - 15s 500ms/step - loss: 0.0534 - categorical_accuracy: 0.9858\n",
      "Epoch 8/20\n",
      "31/31 [==============================] - 15s 486ms/step - loss: 0.0528 - categorical_accuracy: 0.9797\n",
      "Epoch 9/20\n",
      "31/31 [==============================] - 15s 487ms/step - loss: 0.0248 - categorical_accuracy: 0.9939\n",
      "Epoch 10/20\n",
      "31/31 [==============================] - 16s 506ms/step - loss: 0.0188 - categorical_accuracy: 0.9939\n",
      "Epoch 11/20\n",
      "31/31 [==============================] - 19s 612ms/step - loss: 0.0177 - categorical_accuracy: 0.9939\n",
      "Epoch 12/20\n",
      "31/31 [==============================] - 17s 556ms/step - loss: 0.0099 - categorical_accuracy: 0.9959\n",
      "Epoch 13/20\n",
      "31/31 [==============================] - 15s 486ms/step - loss: 0.0063 - categorical_accuracy: 0.9959\n",
      "Epoch 14/20\n",
      "31/31 [==============================] - 15s 477ms/step - loss: 0.0042 - categorical_accuracy: 0.9959\n",
      "Epoch 15/20\n",
      "31/31 [==============================] - 15s 486ms/step - loss: 0.0039 - categorical_accuracy: 0.9959\n",
      "Epoch 16/20\n",
      "31/31 [==============================] - 16s 499ms/step - loss: 0.0034 - categorical_accuracy: 0.9959\n",
      "Epoch 17/20\n",
      "31/31 [==============================] - 15s 498ms/step - loss: 0.0034 - categorical_accuracy: 0.9959\n",
      "Epoch 18/20\n",
      "31/31 [==============================] - 15s 479ms/step - loss: 0.0033 - categorical_accuracy: 0.9959\n",
      "Epoch 19/20\n",
      "31/31 [==============================] - 15s 490ms/step - loss: 0.0031 - categorical_accuracy: 0.9959\n",
      "Epoch 20/20\n",
      "31/31 [==============================] - 16s 503ms/step - loss: 0.0030 - categorical_accuracy: 1.0000\n",
      "7/7 [==============================] - 1s 188ms/step - loss: 0.3161 - categorical_accuracy: 0.9528\n",
      "The accuracy of the neural network on the test dataset is: 0.9528301954269409.\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "batch_size = 16\n",
    "training_epoch = 20\n",
    "\n",
    "improved_model = build_model()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "improved_model.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "\n",
    "improved_model_history = improved_model.fit(X_train, y_train, epochs=training_epoch, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the neural network\n",
    "loss, accuracy = improved_model.evaluate(X_test, y_test)\n",
    "print(f\"The accuracy of the neural network on the test dataset is: {accuracy}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea655f64",
   "metadata": {},
   "source": [
    "## 5. Overfitting Prevention\n",
    "    Early Stopping to Prevent the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53437133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 15s 583ms/step - loss: 0.4077 - categorical_accuracy: 0.8886 - val_loss: 0.1795 - val_categorical_accuracy: 0.9485\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 14s 558ms/step - loss: 0.1768 - categorical_accuracy: 0.9392 - val_loss: 0.1401 - val_categorical_accuracy: 0.9691\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 13s 536ms/step - loss: 0.1423 - categorical_accuracy: 0.9595 - val_loss: 0.1892 - val_categorical_accuracy: 0.9485\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 14s 544ms/step - loss: 0.0862 - categorical_accuracy: 0.9772 - val_loss: 0.1373 - val_categorical_accuracy: 0.9278\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 13s 533ms/step - loss: 0.0622 - categorical_accuracy: 0.9797 - val_loss: 0.2188 - val_categorical_accuracy: 0.9485\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 13s 506ms/step - loss: 0.0630 - categorical_accuracy: 0.9772 - val_loss: 0.1893 - val_categorical_accuracy: 0.9485\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 13s 510ms/step - loss: 0.0472 - categorical_accuracy: 0.9848 - val_loss: 0.1388 - val_categorical_accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Set the best hyper-parameter from previous tuning. (You may change them based on previous results)\n",
    "\n",
    "learning_rate = 0.001\n",
    "# batch_size = 32\n",
    "batch_size = 16\n",
    "training_epoch = 20\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Use the early stopping to prevent the overfitting.\n",
    "    Useful resource: https://keras.io/api/callbacks/early_stopping/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Build model\n",
    "improved_model = build_model()\n",
    "# Define the loss function\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "# Compile the model\n",
    "improved_model.compile(optimizer=optimizer, loss=loss_func, metrics=['categorical_accuracy'])\n",
    "\n",
    "\"\"\" Filling code below \"\"\"\n",
    "\n",
    "# Define the early stopping callback\n",
    "# Monitor = default value val_loss.\n",
    "# Patience = number of epochs with no improvements after which training will be stopped.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\"\"\" End Filling \"\"\"\n",
    "\n",
    "# Train the model with the tuned hyper-parameters and early-stopping.\n",
    "history = improved_model.fit(X_train_hp, y_train_hp, validation_data=(X_val_hp, y_val_hp), \n",
    "                             epochs=training_epoch, batch_size=batch_size, callbacks=[early_stopping])\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b3bcf",
   "metadata": {},
   "source": [
    "## 6. Compare Performance of Basic and Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9e5f2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 186ms/step\n",
      "Performance of the basic model.\n",
      "The accuracy is: 0.8773584905660378.\n",
      "The precision is: 0.4386792452830189.\n",
      "The recall is: 0.5.\n",
      "The f1 score is: 0.46733668341708545.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midrar/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall, F1-score of the basic model\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
    "precision = precision_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='macro')\n",
    "\n",
    "print(\"Performance of the basic model.\")\n",
    "print(f\"The accuracy is: {accuracy}.\")\n",
    "print(f\"The precision is: {precision}.\")\n",
    "print(f\"The recall is: {recall}.\")\n",
    "print(f\"The f1 score is: {f1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cde3fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 203ms/step\n",
      "Performance of the improved model.\n",
      "The accuracy is: 0.9198113207547169.\n",
      "The precision is: 0.8042153377348908.\n",
      "The recall is: 0.8715880893300247.\n",
      "The f1 score is: 0.8326445321569538.\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall, F1-score of the improved model\n",
    "\n",
    "y_pred_improved = improved_model.predict(X_test)\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1))\n",
    "precision = precision_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1), average='macro')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred_improved, axis=1), average='macro')\n",
    "\n",
    "print(\"Performance of the improved model.\")\n",
    "print(f\"The accuracy is: {accuracy}.\")\n",
    "print(f\"The precision is: {precision}.\")\n",
    "print(f\"The recall is: {recall}.\")\n",
    "print(f\"The f1 score is: {f1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e076e65",
   "metadata": {},
   "source": [
    "## I see what you're talking about now. The preceision in the model, before it is improved, was about 87% and the F-score was 47%. However, the improved model has an accuracy of 91% and F1 score of 83%. \n",
    "## Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
