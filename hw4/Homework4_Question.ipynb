{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f14744",
   "metadata": {},
   "source": [
    "# Homework 4:  Further Hyper-parameter Tuning  and visualize performance for event detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b1f53",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "\n",
    "    1. Further Hyper-parameter Tuning for event classification\n",
    "\n",
    "    2. Visualize the loss and accuracy of the train and validation datasets during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorflow, which is a framework for deep learning.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "# Load numpy library as \"np\", which can handle large matrices and provides some mathematical functions.\n",
    "import numpy as np \n",
    "# Load pandas as \"pd\", which is useful when working with data tables. \n",
    "import pandas as pd \n",
    "# Load the SciPy, which provides many useful tools for scientific computing\n",
    "import scipy\n",
    "# Load random, which provide some randomize functions.\n",
    "import random\n",
    "# Load a function pyplot as \"plt\" to plot figures.\n",
    "import matplotlib.pyplot as plt\n",
    "# Load functions to calculate precision, and recall\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Load the VAR function for Vector Autoregression\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Setup the random seed for reproducibility\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The root directory of the pmuBAGE data\n",
    "pmuBAGE_data_dir = \"pmuBAGE/data\"\n",
    "\n",
    "# Number of the tensor for voltage and frequency\n",
    "voltage_tensor_number = 31\n",
    "frequency_tensor_number = 21\n",
    "\n",
    "# Load each tensors of voltage events and concatenate them as a big tensor.\n",
    "voltage_tensor_list = []\n",
    "for idx in range(voltage_tensor_number):\n",
    "    voltage_sub_tensor = np.load(f\"{pmuBAGE_data_dir}/voltage/voltage_{idx}.npy\")\n",
    "    voltage_tensor_list.append(voltage_sub_tensor)\n",
    "voltage_tensor = np.concatenate(voltage_tensor_list, axis=0)\n",
    "\n",
    "\n",
    "# Load each tensors of frequency events and concatenate them as a big tensor.\n",
    "frequency_tensor_list = []\n",
    "for idx in range(frequency_tensor_number):\n",
    "    frequency_sub_tensor = np.load(f\"{pmuBAGE_data_dir}/frequency/frequency_{idx}.npy\")\n",
    "    frequency_tensor_list.append(frequency_sub_tensor)\n",
    "frequency_tensor = np.concatenate(frequency_tensor_list, axis=0)\n",
    "\n",
    "# Transpose the big tensor as (event_idx, timestamp, PMU_idx, measurements)\n",
    "voltage_tensor = np.transpose(voltage_tensor, (0, 3, 2, 1))\n",
    "frequency_tensor = np.transpose(frequency_tensor, (0, 3, 2, 1))\n",
    "\n",
    "# Print the shape of the voltage event\n",
    "print(voltage_tensor.shape)\n",
    "print(frequency_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use standardization to pre-process the pmu time series data.\n",
    "    Input  -> two original tensors: voltage_tensor, frequency_tensor\n",
    "    Output -> two standardized tensors: voltage_tensor_standardized, frequency_tensor_standardized\n",
    "    Requirement Details: \n",
    "        The tensor shape is (number_of_event, timestamps (600), pmus (100), measurements (4))\n",
    "        For each time sequence (Single pmu measurement sequence, 600 timestamps), standardize them by Z-Score\n",
    "        z-score = (x - mean) / std\n",
    "\"\"\"\n",
    "\n",
    "# Voltage tensor\n",
    "\n",
    "voltage_mean = np.mean(voltage_tensor, axis=1)\n",
    "voltage_mean = np.expand_dims(voltage_mean, axis=1)\n",
    "voltage_std = np.std(voltage_tensor, axis=1)\n",
    "voltage_std = np.expand_dims(voltage_std, axis=1)\n",
    "voltage_tensor_standardized = np.nan_to_num((voltage_tensor - voltage_mean) / voltage_std)\n",
    "\n",
    "# Frequency tensor\n",
    "\n",
    "frequency_mean = np.mean(frequency_tensor, axis=1)\n",
    "frequency_mean = np.expand_dims(frequency_mean, axis=1)\n",
    "frequency_std = np.std(frequency_tensor, axis=1)\n",
    "frequency_std = np.expand_dims(frequency_std, axis=1)\n",
    "frequency_tensor_standardized = np.nan_to_num((frequency_tensor - frequency_mean) / frequency_std)\n",
    "\n",
    "print(voltage_tensor_standardized.shape)\n",
    "print(frequency_tensor_standardized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db823b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of the classes\n",
    "num_classes = 2\n",
    "\n",
    "# Number of the voltage and frequency events in the dataset\n",
    "n_voltage = voltage_tensor_standardized.shape[0]\n",
    "n_frequency = frequency_tensor_standardized.shape[0]\n",
    "\n",
    "# Define the labels\n",
    "# Voltage events' label is defined as: 0\n",
    "voltage_label = np.array([0] * n_voltage)\n",
    "# Frequency events' label is defined as: 1\n",
    "frequency_label = np.array([1] * n_frequency)\n",
    "\n",
    "\"\"\"\n",
    "    Implement the one-hot encoding on the lablel of of the voltage and frequency event labels.\n",
    "    Input  -> Original voltage and frequency labels (voltage_label, frequency_label)\n",
    "    Output -> One-hot encoded voltage and frequency labels (voltage_label_onehot, frequency_label_onthot)\n",
    "    Voltage label: \"0\" -> \"[1, 0]\"\n",
    "    Frequency label: \"1\" -> \"[0, 1]\"\n",
    "    You can use any library or tool for doing this\n",
    "\"\"\"\n",
    "\n",
    "voltage_label_onehot = tf.keras.utils.to_categorical(voltage_label, num_classes=num_classes)\n",
    "frequency_label_onthot = tf.keras.utils.to_categorical(frequency_label, num_classes=num_classes)\n",
    "\n",
    "# Should be [1, 0]\n",
    "print(voltage_label_onehot[0])\n",
    "# Should be [0, 1]\n",
    "print(frequency_label_onthot[0])\n",
    "# Should be (620, 2)\n",
    "print(voltage_label_onehot.shape)\n",
    "# Should be (84, 2)\n",
    "print(frequency_label_onthot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68699c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage_tensor_standarded_permuted = voltage_tensor_standardized[np.random.permutation(n_voltage)]\n",
    "frequency_tensor_standarded_permuted = frequency_tensor_standardized[np.random.permutation(n_frequency)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the data to train and test\n",
    "train_portion = 0.7\n",
    "\n",
    "# Samples\n",
    "X_voltage = voltage_tensor_standarded_permuted\n",
    "X_frequency = frequency_tensor_standarded_permuted\n",
    "# Labels\n",
    "y_voltage = voltage_label_onehot\n",
    "y_frequency = frequency_label_onthot\n",
    "\n",
    "\"\"\"\n",
    "    Seperate the samples and labels to train and test datasets.\n",
    "    70% of the voltage and frequency samples and labels are combined as training dataset\n",
    "    30% remainings are combined as testing dataset\n",
    "    Input  -> X_voltage, X_frequency, y_voltage, y_frequency\n",
    "    Output -> X_train, y_train, X_test, y_test\n",
    "        X_train contains 70% of the X_voltage and X_frequency\n",
    "        y_train contains 70% of the y_voltage and y_frequency\n",
    "        X_test contains 30% of the X_voltage and X_frequency\n",
    "        y_test contains 30% of the y_voltage and y_frequency\n",
    "\"\"\"\n",
    "\n",
    "# X_train\n",
    "X_train_voltage = X_voltage[:int(n_voltage * train_portion)] \n",
    "X_train_frequency = X_frequency[:int(n_frequency * train_portion)]\n",
    "X_train = np.concatenate([X_train_voltage, X_train_frequency], axis=0)\n",
    "\n",
    "# y_train\n",
    "y_train_voltage = y_voltage[:int(n_voltage * train_portion)] \n",
    "y_train_frequency = y_frequency[:int(n_frequency * train_portion)]\n",
    "y_train = np.concatenate([y_train_voltage, y_train_frequency], axis=0)\n",
    "\n",
    "# X_test\n",
    "X_test_voltage = X_voltage[int(n_voltage * train_portion):] \n",
    "X_test_frequency = X_frequency[int(n_frequency * train_portion):]\n",
    "X_test = np.concatenate([X_test_voltage, X_test_frequency], axis=0)\n",
    "\n",
    "# y_test\n",
    "y_test_voltage = y_voltage[int(n_voltage * train_portion):] \n",
    "y_test_frequency = y_frequency[int(n_frequency * train_portion):]\n",
    "y_test = np.concatenate([y_test_voltage, y_test_frequency], axis=0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Seperate the Training Dataset to Training (80%) and Validation (20%).\n",
    "    Perform the hyper-parameter tuning to find the best parameter combination.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "val_portaton = 0.2\n",
    "\n",
    "n_voltage_train = int(n_voltage * train_portion)\n",
    "n_frequency_train = int(n_frequency * train_portion)\n",
    "\n",
    "X_train_voltage = X_voltage[:int(n_voltage * train_portion)] \n",
    "X_train_frequency = X_frequency[:int(n_frequency * train_portion)]\n",
    "X_train = np.concatenate([X_train_voltage, X_train_frequency], axis=0)\n",
    "\n",
    "# X_val_hp\n",
    "X_val_hp_voltage = X_train[:int(n_voltage_train * val_portaton)]\n",
    "X_val_hp_frequency = X_train[n_voltage_train: n_voltage_train + int(n_frequency_train * val_portaton)]\n",
    "X_val_hp = np.concatenate([X_val_hp_voltage, X_val_hp_frequency], axis=0)\n",
    "\n",
    "# y_val_hp\n",
    "y_val_hp_voltage = y_train[:int(n_voltage_train * val_portaton)]\n",
    "y_val_hp_frequency = y_train[n_voltage_train: n_voltage_train + int(n_frequency_train * val_portaton)]\n",
    "y_val_hp = np.concatenate([y_val_hp_voltage, y_val_hp_frequency], axis=0)\n",
    "\n",
    "\n",
    "# X_train_hp\n",
    "X_train_hp_voltage = X_train[int(n_voltage_train * val_portaton): n_voltage_train]\n",
    "X_train_hp_frequency = X_train[n_voltage_train + int(n_frequency_train * val_portaton):]\n",
    "X_train_hp = np.concatenate([X_train_hp_voltage, X_train_hp_frequency], axis=0)\n",
    "\n",
    "# y_train_hp\n",
    "y_train_hp_voltage = y_train[int(n_voltage_train * val_portaton): n_voltage_train]\n",
    "y_train_hp_frequency = y_train[n_voltage_train + int(n_frequency_train * val_portaton):]\n",
    "y_train_hp = np.concatenate([y_train_hp_voltage, y_train_hp_frequency], axis=0)\n",
    "\n",
    "\n",
    "X_train, y_train = X_train_hp, y_train_hp\n",
    "X_val, y_val = X_val_hp, y_val_hp\n",
    "\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8cf813",
   "metadata": {},
   "source": [
    "## 1. Further Hyper-parameter Tuning for event classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "        Feel free to change the structure of the model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(600, 100, 4)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Hyper-parameters\n",
    "\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Try different combination of the hyper-parameters. \n",
    "    Using grid search to get the best hyper-parameters\n",
    "    Choose the best hyper-parameter combination to train the final model.\n",
    "    Pick your own hyper-parameters and search range.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b246410",
   "metadata": {},
   "source": [
    "## 2. Visualize the loss and accuracy of the train and validation datasets during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\"\"\"\n",
    "    Using the best hyper-parameters get from the previous step and train the final model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Filling code below \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" End Filling \"\"\"\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\"\"\"\n",
    "    Plot the loss of the training and validation datasets changes during the training.\n",
    "    The information of the loss during the training is stored in \"history\" (return from model.fit()).\n",
    "    Useful resource: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Filling code below \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" End Filling \"\"\"\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------------------------------------##\n",
    "##---------------------Students start filling below----------------------##\n",
    "##-----------------------------------------------------------------------##\n",
    "\n",
    "\"\"\"\n",
    "    Plot the accuracy on the training and validation datasets during the training.\n",
    "    The information of the accuracy during the training is stored in \"history\" (return from model.fit()).\n",
    "    Useful resource: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Filling code below \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" End Filling \"\"\"\n",
    "\n",
    "##-----------------------------------------------------------------------##\n",
    "##------------------------------End filling------------------------------##\n",
    "##-----------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d171ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
